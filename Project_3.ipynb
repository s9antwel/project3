{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNIA 18/19 Project 3:  Regularization and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline: 26. January 2019, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dropout$~$ (9 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you would use an already implemented **feed-forward network** using **only** Numpy. We will add here the dropout regualrization technique that you learned about in the lecture.\n",
    "\n",
    "The goal is to implement dropout for layer 2 in the provided three-layers network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start, here is a simple introduction about dropout. Dropout is a regualization technique that performs random dropping of neurons with a certain probability at each mini-batch in training time. Introducing such randomness or noise at training is helpful to make the network not fully dependent on the training set and therefore, it could generalize better to other unseen data. \n",
    "\n",
    "A simple dropout implementation creates a mask ($r^{(l)}_j$) for every neuron $j$ of the hidden layer $l$ by drawing from a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) with probability $p$, where $p$ is the probability of keeping neurons. You can think of it as a coin flip for each neuron with a probability $p$ to determine if that neuron will be kept or not. For more information, refer to section 7.12 in Deep Learning book.\n",
    "\n",
    "$$ r^{(l)}_j \\sim Bernoulli(p) $$\n",
    "This mask is then applied to the hidden layer output ($h^{(l)}$) to obtain the regularized hidden layer activation $\\hat{h}^{(l)}$\n",
    "$$ \\hat{h}^{(l)} = r^{(l)} * h^{(l)}$$\n",
    "However, such an implementation requires the layer to be multiplied by the dropout coefficient $p$ at evaluation time to balance the larger number of active units during testing.\n",
    "$$ \\hat{h}^{(l)} = p * h^{(l)}$$\n",
    "Such an implementation requires the code to switch between different code blocks for forward-pass evaluation during training and testing. Hence, a smoother way to implement dropout is to use ***inverted dropout*** where the mask generated at the training is multiplied by the inverse of the dropout coefficient.\n",
    "$$ r^{(l)}_j \\sim Bernoulli(p) * \\frac{1}{p}$$\n",
    "This scheme allows the scaling to be learned during training and hence, no switching between code blocks is required. The idea of using inverted dropout is to provide a form of normalization; when dropping out neurons during training, we are now giving the network less neurons that what it will have in the test time. So, dividing by the keep_probs is done to scale the training so that we will not need to do anything in test time. \n",
    "\n",
    "a) The following code implements a simple one-hiddien-layer neural network similar to what you have done in project 2. Update this code to implement inverted dropout for a hidden layer size of 50 neurons (sections to be updated are marked with TODO). Make sure to handle forward and backward passes and handle inference time as well (5.5 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load **Fashion-MNIST** dataset and normalized it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_trainval, Y_trainval), (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_trainval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval = np.reshape(X_trainval, (X_trainval.shape[0],  X_trainval.shape[1] *  X_trainval.shape[2]))\n",
    "print('The X_trainval has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_trainval.shape[0], X_trainval.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.reshape(X_test, (X_test.shape[0],  X_test.shape[1] *  X_test.shape[2]))\n",
    "print('The X_test has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalize the data. Subtract the mean and divide by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_normalization(X_trainval, X_test):\n",
    "    feature_means = X_trainval.mean(axis = 0)\n",
    "    feature_std = np.std(X_trainval, axis = 0)\n",
    "    X_trainval_normalized = (X_trainval - feature_means) / feature_std\n",
    "    X_test_normalized = (X_test - feature_means) / feature_std\n",
    "    return X_trainval_normalized, X_test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The normalization should be done on X_train and X_test\n",
    "# and normalized data should have the exactly same shape as the original data matrix.\n",
    "\n",
    "X_trainval, X_test = data_normalization(X_trainval, X_test)\n",
    "print(X_trainval.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "        \n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l1 : float (default: 0.0)\n",
    "        Lambda value for L1-regularization.\n",
    "        No regularization if l1=0.0 (default)\n",
    "        \n",
    "    l2 : float (default: 0.0)\n",
    "        Lambda value for L2-regularization.\n",
    "        No regularization if l2=0.0 (default)\n",
    "        \n",
    "    epochs : int (default: 500)\n",
    "        Number of passes over the training set.\n",
    "        \n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "        \n",
    "    alpha : float (default: 0.0)\n",
    "        Momentum constant.\n",
    "        \n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "        \n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "        \n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        Normal gradient descent learning if k=1 (default).\n",
    "        \n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "    \n",
    "    dropout : float (default: 1.0, no dropout)\n",
    "        Set the dropout coefficient (keep probability of neurons)\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "      \n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=50, eta=0.001,\n",
    "                 alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=1, dropout = 1.0):\n",
    "\n",
    "        self.n_output = n_output\n",
    "        self.r = np.random.RandomState(random_state)\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.w1, self.w2 = self._initialize_weights()\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        \n",
    "        # The dropout variable is the keep probability for neurons.\n",
    "        # Initialize the class variable \"dropout\" like other variables above. Also, initialize a variable mask to None.\n",
    "        # This will allow sharing dropout information during forward and backward pass of the neural networks. Note \n",
    "        # that the __init__ function has already been modified to include dropout coefficient as an argument. (0.5 points)\n",
    "        \n",
    "        #TODO: Implement\n",
    "\n",
    "    def _encode_labels(self, y, k):    \n",
    "        \"\"\"Encode the labels using one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : y represents target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot array\n",
    "\n",
    "        \"\"\"\n",
    "       \n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for idx, val in enumerate(y):\n",
    "            onehot[val, idx] = 1.0\n",
    "        \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        \n",
    "        w1 = self.r.uniform(-1.0, 1.0, size=self.n_hidden*(self.n_features + 1))\n",
    "        w1 = w1.reshape(self.n_hidden, self.n_features + 1)\n",
    "        w2 = self.r.uniform(-1.0, 1.0, size=self.n_output*(self.n_hidden + 1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        \n",
    "        return w1, w2\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\n",
    "\n",
    "        Uses scipy.special.expit to avoid overflow\n",
    "        error for very small input values z.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        return expit(z)\n",
    "\n",
    "    def _sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the logistic function\"\"\"\n",
    "        \n",
    "        sg = self._sigmoid(z)\n",
    "        return sg * (1.0 - sg)\n",
    "\n",
    "    def _add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        \n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        else:\n",
    "            raise AttributeError('`how` must be `column` or `row`')\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "    def _feedforward(self, X, w1, w2, if_train=True):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "        if_train: if this is during training\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        z3 : array, shape = [n_output_units, n_samples]\n",
    "            Net input of output layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        a1 = self._add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = self._add_bias_unit(a2, how='row')\n",
    "        \n",
    "        # Implement inverted dropout using the class variables dropout and activation variable (a2) for the forward\n",
    "        # pass for the second hidden layer below. To create the mask you will have to use self.r.binomial for \n",
    "        # generating the bernoulli distribution. The mask created here needs to be stored in the appropriate mask\n",
    "        # variable defined in the __init__ function for further use by the backward pass.\n",
    "        # Make sure to handle the inference case (2.5 points)\n",
    "        \n",
    "        # TODO: Implement\n",
    "        \n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self._sigmoid(z3)\n",
    "        \n",
    "        return a1, z2, a2, z3, a3\n",
    "\n",
    "    def _L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        \n",
    "        return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) +\n",
    "                                np.sum(w2[:, 1:] ** 2))\n",
    "\n",
    "    def _L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        \n",
    "        return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() +\n",
    "                                np.abs(w2[:, 1:]).sum())\n",
    "\n",
    "    def _get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        output : array, shape = [n_output_units, n_samples]\n",
    "            Activation of the output layer (feedforward)\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float\n",
    "            Regularized cost.\n",
    "\n",
    "        \"\"\"\n",
    "        np.seterr(divide='ignore')\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        \n",
    "        term1[np.isneginf(term1)] = 0\n",
    "        term2[np.isneginf(term2)] = 0\n",
    "        \n",
    "        cost = np.sum(term1 - term2)\n",
    "        L1_term = self._L1_reg(self.l1, w1, w2)\n",
    "        L2_term = self._L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + L1_term + L2_term\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, shape = [n_hidden_units, n_features]\n",
    "            Gradient of the weight matrix w1.\n",
    "        grad2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Gradient of the weight matrix w2.\n",
    "        \"\"\"\n",
    "        \n",
    "        # backpropagation\n",
    "        sigma3 = a3 - y_enc\n",
    "        z2 = self._add_bias_unit(z2, how='row')\n",
    "        sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2)\n",
    "        \n",
    "        # Implement dropout for the backward pass, use class variables for mask and dropout for this task (2.5 points)\n",
    "        \n",
    "        # TODO: Implement\n",
    "        \n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(a2.T)\n",
    "\n",
    "        # regularize\n",
    "        grad1[:, 1:] += self.l2 * w1[:, 1:]\n",
    "        grad1[:, 1:] += self.l1 * np.sign(w1[:, 1:])\n",
    "        grad2[:, 1:] += self.l2 * w2[:, 1:]\n",
    "        grad2[:, 1:] += self.l1 * np.sign(w2[:, 1:])\n",
    "\n",
    "        return grad1, grad2\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(X.shape) != 2:\n",
    "            raise AttributeError('X must be a [n_samples, n_features] array.\\n'\n",
    "                                 'Use X[:,None] for 1-feature classification,'\n",
    "                                 '\\nor X[[i]] for 1-sample classification')\n",
    "\n",
    "        #TODO: Call the feedforward to get a1, z2, a2, z3, a3 (0.5 point)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, validation_freq=0, X_val=None, y_val=None, data_augmentation=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        y : array, shape = [n_samples]\n",
    "            Target class labels.\n",
    "        print_progress : bool (default: False)\n",
    "            Prints progress as the number of epochs\n",
    "            to stderr.\n",
    "        validation_freq=0, X_val=None, y_val=None: If provided, perform validation check for early stopping (Task 2)\n",
    "        data_augmentation: bool (default: False)\n",
    "            If true, perform data augmentation (Task 3)\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.cost_ = []\n",
    "        train_err = []\n",
    "        test_err = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y, self.n_output)\n",
    "\n",
    "        delta_w1_prev = np.zeros(self.w1.shape)\n",
    "        delta_w2_prev = np.zeros(self.w2.shape)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress:\n",
    "                print('\\nEpoch: %d/%d' % (i+1, self.epochs))\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[:, idx]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3 = self._feedforward(X_data[idx],\n",
    "                                                       self.w1,\n",
    "                                                       self.w2)\n",
    "                cost = self._get_cost(y_enc=y_enc[:, idx],\n",
    "                                      output=a3,\n",
    "                                      w1=self.w1,\n",
    "                                      w2=self.w2)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a1=a1, a2=a2,\n",
    "                                                  a3=a3, z2=z2,\n",
    "                                                  y_enc=y_enc[:, idx],\n",
    "                                                  w1=self.w1,\n",
    "                                                  w2=self.w2)\n",
    "\n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n",
    "                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n",
    "                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n",
    "\n",
    "        return self, train_err, test_err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $6$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Next we are goining to plot the variation of training and test accuracies for variying keep probability values ($p$) denoted by np.arange(0.3, 1.0, 0.3). Explain your findings, how do you observe the effect of keep probability values on accuracies. (3 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: for each dropout value, create a class instance and call the fit function and print the training and test accuracy in percentage for each value of dropout.\n",
    "\n",
    "#Initialize the class for each dropout value as follows:\n",
    "\n",
    "\"\"\"\"\n",
    "nn = MLP(n_output=10, \n",
    "             n_features=X_trainval.shape[1], \n",
    "             n_hidden=50, \n",
    "             l2=0.0, \n",
    "             l1=0.0, \n",
    "             epochs=500, \n",
    "             eta=0.001,\n",
    "             alpha=0.001,\n",
    "             decrease_const=0.00001,\n",
    "             minibatches=50, \n",
    "             shuffle=False,\n",
    "             random_state=1,\n",
    "             dropout = dropout)\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training error and validation error for p values\n",
    "\n",
    "# TODO: Implement\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $3$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Early Stopping$~$ (9 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference see Wikipedia page: https://en.wikipedia.org/wiki/Early_stopping, and also refer to section 7.8 in Deep Learning book.\n",
    "\n",
    "*Early stopping* is a form of regualrization that avoids overfitting by using the validation set error as an indication on when to stop the training before the validation set error starts increasing again. \n",
    "\n",
    "Goal: To study how increasing neurons of a neural network (model complexity) affects the Early Stopping threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) First, update the feedforward neural network code from Task 1 to calculate training and validation error at every 100 iterations (also known as validation frequency) of the training scheme. For this update, you will modify the fit function. The signature of the train function in task 1 include the following default values: (validation_freq=0, X_val=None, y_val=None), notice that these arguments need to be provided to calculate the validation error (of the validation set) at every 100 iterations (validation frequency).\n",
    "Overload the fit function in task 1 to perform the early stopping if these arguments are provided (for information about function overloading, refer to: https://stackabuse.com/overloading-functions-and-operators-in-python. \n",
    "The fit function should return train_err and test_err for each step in validation frequency (that are initialized as empty). (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $3.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Use this code for a different number of neurons (50, 200) and plot the variation of training and validation error for every 100th iteration up to 700 iterations. Label the axes and legends appropriately in the plots. \n",
    "Use the same X_trainval, Y_trainval, X_test, Y_test from task 1.\n",
    "Use Keep probability of dropout to 1 as in default values (no dropout). \n",
    "\n",
    "**Note:** to calculate the validation error we use the test set as a proxy validation set. In general, the correct way to do that would be to keep a separate set for validation and test, the reason for that is that any hyperparameter should be selected based on the validation set, and the test set should be kept for testing the model as unseen data only. Please keep in mind that here we are doing a simplification only for the assignment purpose. \n",
    "\n",
    "(3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: create two classes instances and fit two times with different neurons number\n",
    "\n",
    "#TODO: Plots for training and test errors for the 2 models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $3.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Using these plots, make suggestions for an early stopping criteria for each hidden layer size. (1 point)\n",
    "\n",
    "Answer:\n",
    "\n",
    "d) As the number of neurons are increased, you will observe differences in the early stopping criteria for each hidden layer size. Why do you observe such differences? (1 point)\n",
    "\n",
    "Answer:\n",
    "\n",
    "e) How early stopping can be similar to L2 regularization? (1 point)\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $3.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation$~$ (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the performance of neural networks can significantly increase with increasing training data size, collecting a large dataset is hard and needs a lot of work in annotation. Therefore, data augmentation is a widley used technique in machine learning to artificially increase your training set size. In case of images, data augmentation methods can be scaling, rotations, cropping, or mirroring. In this exercise, we will get familiar with some of these techniques and apply them on the MNIST fashion dataset. \n",
    "\n",
    "For additional reading, you can refer to the following paper (also known as AlexNet) which is one of the papers that revolutionized neural networks applications in image classification and has used data augmentation to increase training data size:\n",
    "\n",
    "Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems. 2012 (http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).\n",
    "\n",
    "**Additional note:** These types of augmentation methods usually work best with a type of neural networks called *Convolutional Neural Networks (CNNs)* becuase these networks can learn features that are invariant under transformations (Wikipedia: https://en.wikipedia.org/wiki/Convolutional_neural_network). However, we will use it here with a simple fully connected network to learn the concept. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for all the next questions, the argument of the function is a 2d Numpy array of 28x28 (one single MNIST example before reshaping)\n",
    "\n",
    "a) One common way of data augmentation in images is flipping, here, we will flip the images across the vertical axis. In the following, implement a function that takes an input image and perform vertical flipping. Call your function with a simple  input image and display both the original and transformed images. (0.5 point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Implement vertical flipping. Use Numpy\n",
    "def flip_img(one_input_image):\n",
    "    return flipped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Another common way is image rotation, here, we will flip the images 90 degrees clock wise. In the following, implement a function that takes a one input image and perform rotation. Call your function with a simple one input image and display both the original and transformed images (0.5 point). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Implement image rotation by 90 degrees clockwise using Numpy\n",
    "def rotate_img(one_input_image):\n",
    "    return rotated_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Perform a function that scales one image by a given scale ratio (>1 to work as a zoom-out), make sure that the scaled image has the same size. Call your function with a simple one input image and a scale ratio of 2, and display both the original and transformed images (0.5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Implement image scaling\n",
    "#For this, you can use an image processing library such as: https://scikit-image.org/ (install if needed)\n",
    "def scale_img(one_input_image, scale_ratio):\n",
    "    return scaled_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Another used method is translation, here, we want to translate our image to the right or the left only, make sure to do zero padding as needed after the translation. Call your function twice, once with a simple one input image and a translation of 5 pixels to the right, and once with the same input image but with a translation of 5 pixels to the left (1 point).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Implement image translation. Use Numpy\n",
    "def translate_img(one_input_image, right_or_left_flag, shift_pixels):\n",
    "    return translated_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Another way of augmentation is introducing random noise to your training set. Implement a function that takes an input image and adds Gaussian noise of mean 0 and std 0.2 to the original image (0.5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Implement adding Gaussian noise. Use Numpy\n",
    "def distort_img(one_input_image):\n",
    "    return distorted_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) What are other advantages of data augmentation beside increasing data size? Elaborate on in which cases each type of data augmentation mentioned above could be effective (scaling, translation, etc.) (1.5 point)\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) There are two ways of performing data augmentation. One is offline augmentation where we apply these augmentations before the training and increase the training set size by a factor (that is usually done for small datasets), another way is to apply these augmentations for each mini-batch online during the training (used for larger datasets). We will use here online augmentation in order to not extensively increase the required computation time.\n",
    "\n",
    "1. Perform a function that takes an input of mini-batch images, and for each image in these images, decide if to perform augmentation or not based on a random flip. \n",
    "\n",
    "2. If augmentation is performed, decide which type of augmentation to use based on another random variable (options: flip, scale, rotate, translate, Gaussian distortion). \n",
    "\n",
    "3. For scaling, generate a random scaling factor (in range from 1 to 2). \n",
    "\n",
    "4. For translation, decide randomly based on another variable if translation is done to left or right, and generate a random transition between 5 and 10 pixels.\n",
    "\n",
    "Note that the number of observation in the batch will be the same.\n",
    "\n",
    "(3.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Implement online_augmentation\n",
    "def online_augmentation(batch_images):\n",
    "    return augmented_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Now use the function (online_augmentation) that you implemented in 7, to edit the (fit) function in the class in Task 1. \n",
    "\n",
    "Note: Use the same X_trainval, Y_trainval, X_test, Y_test from task 1. However, **reshape** the mini-batches to a 2d image shape before applying augmentation and after that reshape it again to the flatted version. The fit function has data_augmentation parameter (default: False), if true, data augmentation should be performed (without early stopping and dropout).\n",
    "\n",
    "(1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Create an instance of the MLP class with 50 neurons. Run the train function for 1500 epochs. Plot the train and test performance over epochs (1.5 point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: create an instrance of the class\n",
    "#TODO: run the fit function\n",
    "#TODO: Plot the train and test performance over epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j) Compare the performance of this version of the network with the vanilla-network that you have created in Task 2 (the early stopping without dropout). Do you see any performance increase. Explain why or why not. Based on this dataset, what form of data augmentation you think would be less effective than the others? (1.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $12.0$\n",
    "**Comments:** None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook (.ipynb file) as the solution. Put the names and student ids of your team members below. **Make sure to submit only 1 solution to only 1 tutor.**\n",
    "\n",
    "- Jane Doe, 123456\n",
    "- Jane Doe, 123456\n",
    "- Jane Doe, 123456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points: 0.0 of 30.0 points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
